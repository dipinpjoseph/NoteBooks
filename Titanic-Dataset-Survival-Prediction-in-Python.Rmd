---
title: "Titanic Dataset - Survival Prediction"
author: "Dipin P Joseph"
date: "7/9/2020"
output:
  prettydoc::html_pretty:
    theme: cayman
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem Statement

In this notebook we are dealing with one of the most popular classification problems - "Predicting survival rate of Titanic passengers based on available characteristics".

This problem is hosted as a Kaggle competition and can be accessed by https://www.kaggle.com/c/titanic.

In short, the goal is to create a binary classification model which predict if a person survives or not in titanic shipwreck based on features like their age, sex, financial situation etc.

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- Notebook Check -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-3600373850128255"
     data-ad-slot="2651335722"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

### Importing Dataset

The titanic dataset is freely available on path - https://www.kaggle.com/c/titanic/data.

```{python}
#! pip3 install kaggle

# Kaggle API stuffs
from kaggle.api.kaggle_api_extended import KaggleApi
api = KaggleApi()
api.authenticate()

# Downloading titanic dataset
api.competition_download_files('titanic')

# Unzip titanic.zip
import zipfile
with zipfile.ZipFile("titanic.zip","r") as zip_ref:
    zip_ref.extractall("titanic_data")
    
# Loading Dataset
#! pip3 install pandas
import pandas as pd
pd.set_option('display.expand_frame_repr', False)

df_train = pd.read_csv('titanic_data/train.csv')
df_test = pd.read_csv('titanic_data/test.csv')
print(df_train.head())
print(df_test.head())
```

## Exploratory Data Analysis

Here, we look into the train data more deeply. We starts with Summary statistics, NA resolve, Correlation check and so on.

### Data Summary

```{python}
# Summary of Data
df_train.describe()
```
The range of _Fare_ is huge and we may assume 0.0 Fare was for the ship's staffs.

### Visualization - Relationship between predictors and predicted variable.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

# Passenger Class vs Survival
sns.countplot(x='Pclass',hue='Survived', data=df_train).set_title("Passenger Class vs Survival")
plt.show()
# Passenger Sex vs Survival
sns.countplot(x='Sex',hue='Survived', data=df_train).set_title("Passenger Sex vs Survival")
plt.show()
```

Some observations from the above visualizations,

* 1st class passengers had greater chances of survival while class 3 passengers had the least.    
* Comparing passenger sex, female passengers had a greater chance of survival than their counterparts.

### Data Pre-processing

### Visualization of Missing Values.

```{python}
# Visualization - Missing Values

# Passenger Ids from 892 are part of test data.
df_train = pd.concat([df_train, df_test])
df_train.index = df_train.PassengerId

# Columns with NAs
print(df_train.columns[df_train.isna().any()].tolist())

#! pip3 install missingno
import missingno as msno
# %matplotlib inline

msno.matrix(df_train)
```

From the plot, we could see that columns _Age_ and _Cabin_ posses a lot of NAs. _Age_ seems important and let's look into suitable NA imputation steps.

Let's check missingness in _Embarked_ and remove corresponding rows according to the result.

```{python}
# Filling Fare with median
df_train.Fare.fillna(df_train.Fare.median(), inplace=True)

# Rows with missing values for Embarked
print(df_train.loc[~df_train['Embarked'].isin(['S','Q','C'])])

# Fill those two rows with most applicable value
df_train.Embarked.fillna('C', inplace=True)
```

Let's check the influence of _Pclass_ and _Sex_ on _Age_ by box plot visualizations.


```{python}
# Boxplot of Age on Ticket and Sex categories
df_train.boxplot(column='Age',by=['Pclass','Sex'])
```

From the figure a general inference is that female passengers are younger than males and parameter age is closely related to ticket class.
Median based imputing seems to be fit in this case. We will split train data into 6 categories and finds median on each category. Later these values will replace NAs in _Age_.


```{python}
# Fill NAs with median of each group
df_train['Age'] = df_train.groupby(['Pclass','Sex'])['Age'].apply(lambda x: x.fillna(x.median()))
print((df_train[df_train.Age.isna()].index))
df_train.head()
```
## Feature Engineering

Columns _SibSp_ and _Parch_ gives info about family members onboard. We can unify these to create ne column _family_mem_.

```{python}
# New column family_mem = sum of siblings+parents/children+self
df_train['family_mem'] = df_train.SibSp+df_train.Parch+1
df_train.head()
```
If you notice __Name__ column carefully, honorifics like Mr., Col. were given along with Passenger names. Honorifics information could be added as a separate column and let's look how designation matters in survival.

```{python}
df_train['Hon'] = (df_train['Name'].str.extract(r"\,(.[a-zA-Z]+)\."))
df_train.head()

sns.countplot(x='Hon',hue='Survived', data=df_train).set_title("Passenger Honorifics vs Survival")
plt.show()
```

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- Notebook Check -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-3600373850128255"
     data-ad-slot="2651335722"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>


```{python}
print(list(df_train))
```

### Numeric to Categorical/Factors

Age in our datase is actually ordinal but treated as numeric at the moment. We will look how age is distributed and convert ages to different categories.

```{python}
# Histogram of Age column
df_train['Age'].plot(kind='hist')
plt.show()
```
It seems majority of passengers belongs to age group 20-30. Let's divide age range(0-80) to four categories: 0-20, 21-40, 41-60, 61-80.
```{python}
bins = [0, 10, 20, 30, 40, 50, 60, 70, 80]
names = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60','61-70', '71-80']

df_train['AgeRange'] = pd.cut(df_train['Age'], bins, labels=names)

# Categorize Pclass and Hon aswell
df_train.Pclass = pd.Categorical(df_train.Pclass)
df_train.Hon = pd.Categorical(df_train.Hon)

print (df_train.dtypes)
```

Columns _Name_, _Age_, _SibSp_, _Parch_, _Ticket_, _Cabin_ could be dropped since they are not adding any significant information.

```{python}
# Dropping unnecessary columns
#df_train.drop([ 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)
# Dropping Sex col as Hon has similar but precise information.
df_train.drop([ 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Sex'], axis=1, inplace=True)
df_train.head(3)
```

### Train/Test Split

Here, we will detach test data (PassengerIds from 892) to different dataframe. The step is required to avoid data leakage issue in later stages.

```{python}
# Rows from 893 onwards belongs to test data

df_test = df_train.iloc[891:]
df_train = df_train.iloc[:891]

df_train.drop('PassengerId', axis=1, inplace=True)
df_test.drop(['PassengerId','Survived'], axis=1, inplace=True)
```
### Scaling Numeric Variables 

```{python}
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

df_train[['Fare','family_mem']] = scaler.fit_transform(df_train[['Fare','family_mem']])
#df_train = df_train.round({'Fare': 4, 'family_mem': 4})
df_test[['Fare','family_mem']] = scaler.fit_transform(df_test[['Fare','family_mem']])
#df_test = df_test.round({'Fare': 4, 'family_mem': 4})
```

### Encoding categorical variables

```{python}
df_train = pd.get_dummies(df_train, drop_first=True)
print(df_train.head())

df_test = pd.get_dummies(df_test, drop_first=True)
```
## Modelling

### Simple Logistic Regression

```{python}
Y = df_train.Survived
X = df_train.drop(['Survived'], axis=1)
Y.tail()

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression().fit(X, Y)
pred_log = clf.predict(df_test).astype(int)
```
```{python}
#pd.DataFrame(pred_log).head()
pred_log_df =  pd.DataFrame()
pred_log_df['PassengerId'] = df_test.index
pred_log_df['Survived'] = pred_log
pred_log_df.head()
```
### Cross Validation and generation of different classification models

```{python}
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
import xgboost
from sklearn.model_selection import cross_val_score

clf = svm.SVC(kernel='linear', C=1)
scores = cross_val_score(clf, X, Y, cv=5)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
clf = LogisticRegression()
scores = cross_val_score(clf, X, Y, cv=5)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
clf = RandomForestClassifier(max_depth=2, random_state=0)
scores = cross_val_score(clf, X, Y, cv=5)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
clf = KNeighborsClassifier(n_neighbors=3)
scores = cross_val_score(clf, X, Y, cv=5)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
clf = GaussianNB()
scores = cross_val_score(clf, X, Y, cv=5)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
clf = xgboost.XGBClassifier()
scores = cross_val_score(clf, X, Y, cv=5)
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
```


<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- Notebook Check -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-3600373850128255"
     data-ad-slot="2651335722"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

### XGBoost Classification

```{python}
clf = xgboost.XGBClassifier().fit(X, Y)
pred_xg = clf.predict(df_test).astype(int)

#pd.DataFrame(pred_xg).head()
pred_xg_df =  pd.DataFrame()
pred_xg_df['PassengerId'] = df_test.index
pred_xg_df['Survived'] = pred_xg
pred_xg_df.head()
```

### SVM Classifier

```{python}
clf = svm.SVC(kernel='linear', C=1).fit(X, Y)
pred_svm = clf.predict(df_test).astype(int)

#pd.DataFrame(pred_svm).head()
pred_svm_df =  pd.DataFrame()
pred_svm_df['PassengerId'] = df_test.index
pred_svm_df['Survived'] = pred_svm
pred_svm_df.head()
```


## Kaggle Submission

```{python}
pred_log_df.to_csv('lg_submission.csv', index=False)
pred_xg_df.to_csv('xg_submission.csv', index=False)
pred_svm_df.to_csv('svm_submission.csv', index=False)
```

