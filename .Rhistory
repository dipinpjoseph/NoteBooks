}
# Detected outliers from mahalanobis distance method
te_mh <- te[rows_mh, ]
# Cook's Distance
# Creating a linear model with price as outcome and remaining numeric variables as predictors
lmod <-
glm(formula = as.formula(paste("price ~ ", paste(cols_num, collapse = " + "))),
data = pr_mh,
family = gaussian)
# Calculating Cook's distance
dc <- cooks.distance(lmod)
thresh <-
4 * mean(dc)  # Standard way of selecting threshold. 4 times mean value
# Data for plotting
dfcd <- data.frame(dc, id = 1:length(dc) / length(dc))
# Fetching row numbers of outlier observations
rows_ck <- c()
for (i in 1:length(dc)) {
if (dc[i] > thresh) {
rows_ck = c(rows_ck, i)
}
}
# Detected outliers by Cook's distance
te_ck <- te[rows_ck, ]
# LOF
# Selecting all numeric columns
pr_lof <- te[cols_num_all]
# Size of neighbourhood - 5
d <- dbscan::lof(pr_lof, k = 5)
pr_lof$distance <- d
pr_lof <- pr_lof[order(d, decreasing = TRUE), ]
# Distance 1.75 gives optimum outliers
pr_lof <- pr_lof[pr_lof$distance > 1.75, ]
# Saving row names of outlier obs
rows_lof <- rownames(pr_lof)
# DB Scan
rec_db <-
recipe(as.formula(paste("price ~ ", paste(cols_num, collapse = " + "))), data = tr) %>%
step_naomit(everything()) %>%  # Not needed because of lack in missing data
step_nzv(all_predictors()) %>%  # remove near zero variance predictor variables
step_lincomb(all_predictors()) %>%   # remove predictors that are linear combinations of other predictors
prep(data = tr)
pr_db <- bake(rec_db, te)
# From Elbow method value of eps was found to be 25000
clustered <- dbscan::dbscan(pr_db, eps = 25000, minPts = 4)
# Fetching outlier observations
pr_db_te <- te[clustered$cluster == 0, ]
# Saving row names of outlier obs
rows_db <- rownames(pr_db_te)
# Outliers from 5 point summary after YJ transform
b_df = yj_t
rows_five <- c()
for (k in 1:length(cols_num_all)) {
dfk <- data.frame(y = b_df[[cols_num_all[k]]])
fnum = (fivenum(dfk$y))
# Calculating end point of whiskers
low = fnum[2] - 1.5 * (fnum[4] - fnum[2])
high = fnum[4] + 1.5 * (fnum[4] - fnum[2])
for (i in 1:nrow(dfk)) {
# Checking if the value is novel
if (dfk$y[i] > high | dfk$y[i] < low) {
rows_five = c(rows_five, i)
}
}
}
# Saving outlier rows from 5-point summary
out_five = unique(rows_five)
# Z Score
rows_z <- c()
for (k in 1:length(cols_num_all)) {
dfk <- data.frame(y = df_t[[cols_num_all[k]]])
dfk <- scale(dfk)
for (i in 1:length(dfk)) {
# Z-score of point outside (-3,3) is considered as outlier
if (dfk[i] > 3 | dfk[i] < -3) {
rows_z = c(rows_z, i)
}
}
}
# Fetching row numbers and observations of outlier observations
rows_z = unique(rows_z)
# Outliers from 5 point summary for Raw Data
b_df = df_t
rows_5_raw <- c()
for (k in 1:length(cols_num_all)) {
dfk <- data.frame(y = b_df[[cols_num_all[k]]])
fnum = (fivenum(dfk$y))
# Calculating end point of whiskers
low = fnum[2] - 1.5 * (fnum[4] - fnum[2])
high = fnum[4] + 1.5 * (fnum[4] - fnum[2])
for (i in 1:nrow(dfk)) {
# Checking if the value is novel
if (dfk$y[i] > high | dfk$y[i] < low) {
rows_5_raw = c(rows_5_raw, i)
}
}
}
# Saving outlier rows from 5-point summary
rows_5_raw = unique(rows_5_raw)
# Support Vector Machines
# Detecting outliers in training set
rows_tr <- c()
# Using Z -score method to find outliers in training observations
for (k in 1:length(cols_num_all)) {
dfk <- data.frame(y = tr[[cols_num_all[k]]])
dfk <- scale(dfk)
for (i in 1:length(dfk)) {
if (dfk[i] > 3 | dfk[i] < -3) {
rows_tr = c(rows_tr, i)
}
}
}
rows_tr = unique(rows_tr)
# nu calculation
nu_val = length(rows_tr) / (length(tr$price) - length(rows_tr))
# Trains 1-class SVM with numeric training observations
model <-
e1071::svm(
tr[cols_num_all],
y = NULL,
type = 'one-classification',
nu = nu_val,
scale = TRUE,
kernel = "radial"
)
# Predicting outliers on test numeric variables
good <- predict(model, te[cols_num_all])
svm_out <- te[!good, ]
# Saving rownames of outliers
rows_svm <- rownames(svm_out)
# Robust Methods
getMethods <- function() {
mi <- caret::getModelInfo()
Label <- vector(mode = "character", length = length(mi))
Tags <- vector(mode = "character", length = length(mi))
Regression <- vector(mode = "logical", length = length(mi))
Classification <- vector(mode = "logical", length = length(mi))
for (row in 1:length(mi)) {
Label[row] <- mi[[row]]$label
Tags[row] <- paste(collapse = ", ", mi[[row]]$tags)
Regression[row] <- "Regression" %in% mi[[row]]$type
Classification[row] <- "Classification" %in% mi[[row]]$type
}
data.frame(
Model = names(mi),
Tags,
Regression,
Classification,
stringsAsFactors = FALSE
)
}
methods <- getMethods()
rb_methods <-
methods[grepl(methods$Tags, pattern = ".*Robust Model.*", ignore.case = TRUE),]
# Integration of outlier observations - Some methods provide row numbers while other give row names
# Row indices
rows_out_int <- out_five
#rows_out_int <- intersect(rows_out_int, rows_z)
rows_out_int <- intersect(rows_out_int, rows_mh)
rows_out_int <- intersect(rows_out_int, rows_ck)
# Outlier observation intersection - Part 1
te_int_out <- te[rows_out_int, ]
# Row Names
rows_out_ch <- rows_db
rows_out_ch <- intersect(rows_out_ch, rows_lof)
rows_out_ch <- intersect(rows_out_ch, rows_svm)
# Outlier observation intersection - Part 2
te_ch_out <- te[rows_out_ch, ]
# Intersection of outlier observations
te_out_final = inner_join(te_int_out, te_ch_out)
compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"]
compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"][1]
rownames(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"])
row_number(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"])
row.names(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"])
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
rows_5_raw
rows_z
rows_out_raw
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"][1]
type(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"])
typeof(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"])
unlist(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"])
df <- data.frame(matrix(unlist(compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]), nrow=length(compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]), byrow=T))
df <- data.frame(matrix(unlist(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"]), nrow=length(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"]), byrow=T))
df.head()
head(df)
df <- data.frame(matrix(unlist(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"]), nrow=length(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"]), byrow=F))
head(df)
str(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"])
str(df)
str(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"])
str(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"][1][1])
str(compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"]$)
bg <- compute.bagplot(df_t['price'], df_t['age'])["pxy.outlier"]
str(bg)
bg$pxy.outlier
str(bg$pxy.outlier)
str(bg$pxy.outlier[2])
str(bg$pxy.outlier[1])
str(bg$pxy.outlier[1])
str(bg$pxy.outlier[100])
str(bg$pxy.outlier[138])
str(bg$pxy.outlier[500])
str(bg$pxy.outlier[250])
str(bg$pxy.outlier[200])
length(bg$pxy.outlier[200])
length(bg$pxy.outlier)
str(bg$pxy.outlier)
str(bg$pxy.outlier['age'])
str(bg$pxy.outlier['price'])
str(bg$pxy.outlier$price)
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
length(bg$pxy.outlier)
df_t[10222 %in% df_tprice,  ]
df_t[10222 %in% df_t$price,  ]
df_t[4 %in% df_t$rooms,  ]
rownames(df_t[4 %in% df_t$rooms,  ])
length(bg$pxy.outlier)
bg$pxy.outlier[1]
# Using Z -score method to find outliers in training observations
for (k in cols_num_all) {
for (l in cols_num_all){
df_med = compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]
len_bag = len(df_med$pxy.outlier)
for (i in 1:length(len_bag/2)){
if(df_med$pxy.outlier[i] %in% df_t[k]){
print("Success")
}
}
}
}
rows_bag_raw <- c()
# Using Z -score method to find outliers in training observations
for (k in cols_num_all) {
for (l in cols_num_all){
df_med = compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]
len_bag = length(df_med$pxy.outlier)
for (i in 1:length(len_bag/2)){
if(df_med$pxy.outlier[i] %in% df_t[k]){
print("Success")
}
}
}
}
rows_bag_raw = unique(rows_bag_raw)
df[which(df_t$age == 10, ]
df[which(df_t$age == 10), ]
which(df_t$age == 10)
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
(which(df_t[k] == df_med$pxy.outlier[1])
)
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
print(rows_bag_raw)
get_r <- which(df_t[['age']] == 4)
str(get_r)
typeof(get_r)
typeof(get_r[3])
get_r[3]
unlist(get_r, use.names=FALSE)
unlist(get_r, use.names=FALSE)[4]
typeof(unlist(get_r, use.names=FALSE))
rows_bag_raw <- c()
# Using Z -score method to find outliers in training observations
for (k in cols_num_all) {
for (l in cols_num_all){
df_med = compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]
len_bag = length(df_med$pxy.outlier)
for (i in 1:length(len_bag/2)){
get_r <- which(df_t[[k]] == df_med$pxy.outlier[i])
print(str(get_r))
print(get_r)
rows_bag_raw <- c(unlist(get_r, use.names=FALSE), rows_bag_raw)
}
}
}
rows_bag_raw = unique(rows_bag_raw)
print(rows_bag_raw)
rows_bag_raw
rows_bag_raw <- c()
# Using Z -score method to find outliers in training observations
for (k in cols_num_all[2]) {
for (l in cols_num_all){
df_med = compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]
len_bag = length(df_med$pxy.outlier)
for (i in 1:length(len_bag/2)){
get_r <- which(df_t[[k]] == df_med$pxy.outlier[i])
#print(str(get_r))
print(get_r)
rows_bag_raw <- c(unlist(get_r, use.names=FALSE), rows_bag_raw)
}
}
}
rows_bag_raw = unique(rows_bag_raw)
print(rows_bag_raw)
rows_bag_raw <- c()
# Using Z -score method to find outliers in training observations
for (k in cols_num_all) {
for (l in cols_num_all){
df_med = compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]
len_bag = length(df_med$pxy.outlier)
for (i in 1:length(len_bag/2)){
get_r <- which(df_t[[k]] == df_med$pxy.outlier[i])
#print(str(get_r))
print(get_r)
rows_bag_raw <- c(unlist(get_r, use.names=FALSE), rows_bag_raw)
}
}
}
rows_bag_raw = unique(rows_bag_raw)
print(rows_bag_raw)
str(rows_bag_raw)
rows_bag_raw <- c()
# Using Z -score method to find outliers in training observations
for (k in cols_num_all) {
for (l in cols_num_all){
df_med = compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]
len_bag = length(df_med$pxy.outlier)
for (i in 1:length(len_bag/2)){
get_r <- unique(which(df_t[[k]] == df_med$pxy.outlier[i]),which(df_t[[k]] == df_med$pxy.outlier[i]))
#print(str(get_r))
print(get_r)
rows_bag_raw <- c(unlist(get_r, use.names=FALSE), rows_bag_raw)
}
}
}
rows_bag_raw = unique(rows_bag_raw)
print(rows_bag_raw)
rows_bag_raw <- c()
# Using Z -score method to find outliers in training observations
for (k in cols_num_all) {
for (l in cols_num_all){
df_med = compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]
len_bag = length(df_med$pxy.outlier)
for (i in 1:length(len_bag/2)){
get_r <- unique(which(df_t[[k]] == df_med$pxy.outlier[i]),which(df_t[[l]] == df_med$pxy.outlier[i+len_bag/2]))
#print(str(get_r))
print(get_r)
rows_bag_raw <- c(unlist(get_r, use.names=FALSE), rows_bag_raw)
}
}
}
rows_bag_raw = unique(rows_bag_raw)
print(rows_bag_raw)
rows_bag_raw <- c()
# Using Z -score method to find outliers in training observations
for (k in 'price') {
for (l in 'age'){
df_med = compute.bagplot(df_t[k], df_t[l])["pxy.outlier"]
len_bag = length(df_med$pxy.outlier)
for (i in 1:length(len_bag/2)){
get_r <- unique(which(df_t[[k]] == df_med$pxy.outlier[i]),which(df_t[[l]] == df_med$pxy.outlier[i+len_bag/2]))
#print(str(get_r))
print(get_r)
rows_bag_raw <- c(unlist(get_r, use.names=FALSE), rows_bag_raw)
}
}
}
rows_bag_raw = unique(rows_bag_raw)
print(rows_bag_raw)
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
runApp('MEGA/SEM1-Feb-2020/DSI/Ass2/ReWork')
?data(iris)
str(iiris)
str(iris)
str(cars)
str(mtcars)
?data(mtcars)
?mtcars
str(mtcars)
?str()
?summary()
summary(mtcars)
setwd("~/MEGA/WorkSpace/NoteBooks")
install.packages("prettydoc")
?wine
??wine
str(data(wine))
str(data(wine))
install.packages("gclus")
library("gclus")
str(data(wine))
str(data(wine))
str(wine)
install.packages("Ecdat")
knitr::opts_chunk$set(echo = TRUE)
tr
---
title: "Outlier Detection in R"
author: "Dipin P Joseph"
date: "26/04/2020"
output:
prettydoc::html_pretty:
theme: cayman
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
In this post we will go through different outlier detection methods and topics like,
* Power Transformation (Yeo Johnson Transform) and Density Plots
* Box Plots and 5 Point Summary Statistics
* Z-Score
* Histograms and Scatter Plots
* Mosaic Plots and Bag Plots
* Mahalanobis Distance
* Cook's Distance
* DBScan
* Local Outlier Factors
* 1-class Support Vector Machine
Dataset under consideration - **Males** (from package - _Ecdat_)
First, we will explore more about data using __str()__, __summary()__ methods,
```{r}
library(Ecdat)
summary(Males)
str(Males)
```
## Train-Test Split
Here, we will split data to training and testing set. 70% will go for training and 30% for testing.
```{r}
set.seed(7)
# 70% of the sample size
smp_size <- floor(0.7 * nrow(Males))
# Train and Test Split
train_ind <- sample(seq_len(nrow(Males)), size = smp_size)
tr <- Males[train_ind,]
te <- Males[-train_ind,]
```
## Yeo-Johnson Transformation
Most of the methods will only work well with normalized data. So, we have to transform our variables to another form which are more normally distributed. Yeo-Johnson tansformation will help us here,
```{r}
library(plotly)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
head(Males)
cols <- colnames(Males)
1
cols <- colnames(Males)
cols_num <- cols[1,2,3,4]
cols <- colnames(Males)
cols_num <- cols[1:4]
cols_num
cols_cat <- cols[5:8,10:12]
cols_cat <- cols[5:8:10:12]
cols_cat <- cols[5:8]
cols_cat
cols_cat <- c(cols[5:8],cols[10:12])
cols_cat
density(te$nr)
te
head(te)
?Males
cols_num
cols_num <-cols[3:4)
cols_num <-cols[3:4])
cols_num <-cols[3:4]
cols_num
density(te$exper)
unlist(density(te$exper))
str(density(te$exper))
density(te$exper)[x]
density(te$exper)[1]
density(te$exper)[2]
density(te$exper)[3]
de_b <- density(te$exper)
after <- plot_ly(de_b, x = ~x, y = ~y, type = 'scatter', mode = 'lines')
de_b <- density(te$exper)
after <- plot_ly(x = ~de_b[1], y = ~de_b[2], type = 'scatter', mode = 'lines')
after
de_b <- density(te$exper)
de_b <- data.frame(x = de_b$x, y = de_b$y)
de_a <- density(yj_t$exper)
before <- plot_ly(data=de_b, x = ~x, y = ~y, type = 'scatter', mode = 'lines')
before
cols
cols[6]
cols[8]
te$school
library(plotly)
library(recipes)
# Performing Yeo Johnson transform for Normalization
# Setting variable price as outcome and remaining numerics as predictors
yj_estimates <-
recipe(as.formula(paste("wage ~ ", paste(cols_num, collapse = " + "))), data = tr) %>%
# Power transformation step
step_YeoJohnson(all_numeric()) %>%
# Feeds training data
prep(data = tr)
# The trained process is run of test set
yj_t <- bake(yj_estimates, te)
fnum = (fivenum(yj_t$wage))
print(fnum)
# Calculating end point of whiskers
low = fnum[2] - 1.5 * (fnum[4] - fnum[2])
high = fnum[4] + 1.5 * (fnum[4] - fnum[2])
fnum
print("Outliers are outside region",low,high)
fnum = (fivenum(yj_t$wage))
print(fnum)
# Calculating end point of whiskers
low = fnum[2] - 1.5 * (fnum[4] - fnum[2])
high = fnum[4] + 1.5 * (fnum[4] - fnum[2])
print("Outliers are outside region",low,high)
print(paste("Outliers are outside region",low,high))
